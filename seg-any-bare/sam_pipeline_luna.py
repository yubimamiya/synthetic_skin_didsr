# -*- coding: utf-8 -*-
"""SAM_pipeline_luna.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VIiAdbJEfl8tSZpV3C5zaftJHHi1_6_R

# Object masks from prompts with SAM

The Segment Anything Model (SAM) predicts object masks given prompts that indicate the desired object. The model first converts the image into an image embedding that allows high quality masks to be efficiently produced from a prompt.

The `SamPredictor` class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the `set_image` method, which calculates the necessary image embeddings. Then, prompts can be provided via the `predict` method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction.

## Environment Set-up
"""

using_colab = False

"""## Set-up

Necessary imports and helper functions for displaying points, boxes, and masks.
"""

import numpy as np
import torch
import matplotlib.pyplot as plt
import cv2
import pandas as pd
import os
import PIL
from PIL import Image

def show_mask(mask, ax, random_color=False):
    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        color = np.array([30/255, 144/255, 255/255, 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

def show_points(coords, labels, ax, marker_size=375):
    pos_points = coords[labels==1]
    neg_points = coords[labels==0]
    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)
    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)

def show_box(box, ax):
    x0, y0 = box[0], box[1]
    w, h = box[2] - box[0], box[3] - box[1]
    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))

"""## Load images"""

# EDIT: change name of synthetic_metadata
data_dir = '/projects01/VICTRE/yubi.mamiya/minimal-diffusion/synth_data/'
synthetic_metadata = pd.read_csv(data_dir + 'synthetic_metadata_600.csv', delimiter=',')
# synthetic_metadata.head()

"""## Selecting objects with SAM

First, load the SAM model and predictor. Change the path below to point to the SAM checkpoint. Running on CUDA and using the default model are recommended for best results.

Process the image to produce an image embedding by calling `SamPredictor.set_image`. `SamPredictor` remembers this embedding and will use it for subsequent mask prediction.
"""

import sys
sys.path.append("..")
from segment_anything import sam_model_registry, SamPredictor

sam_checkpoint = "sam_vit_h_4b8939.pth"
sam_checkpoint_path = "/projects01/VICTRE/yubi.mamiya/segment-anything/model_ckpt/sam_vit_h_4b8939.pth"
model_type = "vit_h"

device = "cuda"

# sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
# i think i need the full path to the model checkpoint
sam = sam_model_registry[model_type](checkpoint=sam_checkpoint_path)
sam.to(device=device)

predictor = SamPredictor(sam)

"""Set variables for point prompt

To select the truck, choose a point on it. Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point). Multiple points can be input; here we use only one. The chosen point will be shown as a star on the image.
"""

width = 389
height = 389
center_x = width // 2
center_y = height // 2
total_area = width * height

# selection input point to be center of 389 x 389 image
input_point = np.array([[center_x, center_y]])
input_label = np.array([1])

"""Loop through images and process them using predictor

Predict with `SamPredictor.predict`. The model returns masks, quality predictions for those masks, and low resolution mask logits that can be passed to the next iteration of prediction.

With `multimask_output=True` (the default setting), SAM outputs 3 masks, where `scores` gives the model's own estimation of the quality of these masks. This setting is intended for ambiguous input prompts, and helps the model disambiguate different objects consistent with the prompt. When `False`, it will return a single mask. For ambiguous prompts such as a single point, it is recommended to use `multimask_output=True` even if only a single mask is desired; the best single mask can be chosen by picking the one with the highest score returned in `scores`. This will often result in a better mask.

- Thresholds for masks
- mask_areas ratio should be > 0.02 and < 0.6
- Double check these thresholds with other examples
- This applied for mel_samp_16 and mel_samp_11
- fine tuned with mel_samp_14 to lower lower threshold
- take the highest score out of the filtered masks
"""

SAM_metadata = pd.DataFrame(columns=["image_id"])

for image_ID in synthetic_metadata["image_id"]:
    nameimage_path = os.path.join(data_dir + 'ITA_images' +  '/' + image_ID +'.png')
    if os.path.exists(nameimage_path):
        image = cv2.imread(nameimage_path, cv2.IMREAD_COLOR)
    else:
        print("image path: ", nameimage_path)
        print("error: image can't be found in data directory")

    mask_path = os.path.join(data_dir + 'ITA_masks' +  '/' + image_ID + '_masks' +'.png')

    # get image
    predictor.set_image(image)

    # make prediction of masks
    masks, scores, logits = predictor.predict(
      point_coords=input_point,
      point_labels=input_label,
      multimask_output=True,
    )

    # initialize variables
    best_mask_idx = 0 # do I need the index?
    best_score = 0
    best_mask = None

    # iterate through masks
    for i, (mask, score) in enumerate(zip(masks, scores)):

      # calculate area of masked lesions
      mask_area = 0
      total_area = mask.shape[0] * mask.shape[1]

      for y in range(mask.shape[0]):
        for x in range(mask.shape[1]):
          if mask[x, y] == 1:
            # the masked lesion area is encoded as 1
            mask_area += 1
      mask_area_ratio = mask_area / total_area

      # save the best mask within thresholds
      if mask_area_ratio > 0.02 and mask_area_ratio < 0.6:
        if score > best_score:
          best_mask_idx = i
          best_score = score
          best_mask = mask

    # skip image if there are no good matches
    if best_mask is None:
      print(f"No good masks found for image")
      continue

    # using cv2 to save image to mask_path directory
    # this returns a mask where the lesion region is in white and the background is in black
    cv2.imwrite(mask_path, (best_mask * 255))

    # update metadata df
    SAM_metadata = SAM_metadata._append({"image_ID": image_ID}, ignore_index=True)

# save updated metadata df
# EDIT: create a new name of metadata
SAM_metadata.to_csv(data_dir + "SAM_metadata_600.csv", index=False)